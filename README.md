# ðŸ§µ WGAN-GP on FashionMNIST

An implementation of **Wasserstein GAN with Gradient Penalty (WGAN-GP)** trained on the **FashionMNIST** dataset.  
This project demonstrates stable GAN training, image generation, loss curve visualization, and evaluation using **FrÃ©chet Inception Distance (FID)**.

---

## ðŸ“‚ Project Structure

â”œâ”€â”€ data_preprocessing.py # Dataset loading & normalization
â”œâ”€â”€ model.py # Generator & Discriminator
â”œâ”€â”€ train.py # Training loop (WGAN-GP)
â”œâ”€â”€ evaluation.py # FID computation
â”œâ”€â”€ visualization.py # Save samples, plot loss curves, make GIFs
â”œâ”€â”€ main.py # Run training & evaluation
â”œâ”€â”€ results/ # Generated samples, loss curves, GIFs


---

## âœ¨ Features

- âœ… **WGAN-GP loss** â†’ stable GAN training  
- âœ… **FashionMNIST dataset** (grayscale clothing items)  
- âœ… **Generator & Discriminator** with InstanceNorm  
- âœ… **Visualization**:
  - Save generated samples per epoch  
  - Track loss curves  
  - GIF animations of training progress  
- âœ… **Evaluation**:
  - Compute **FID score** to measure quality/diversity  

---


ðŸ“Œ Requirements

Python 3.8+

PyTorch

torchvision

matplotlib

imageio

numpy

## ðŸš€ Usage

Train the WGAN-GP on **FashionMNIST**:

```bash
python main.py


Outputs:


### Generated Samples During Training
![Training Progress](results/training_progress.gif)
### Loss Curve Progress
![Loss Curve Progress](results/loss_curve_progress.gif)


Evaluation

We use FrÃ©chet Inception Distance (FID) as a metric:

Lower FID = Better quality & diversity